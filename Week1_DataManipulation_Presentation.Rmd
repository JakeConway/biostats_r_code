---
title: "Week 1"
author: "Alexander Frieden"
date: "January 9, 2016"
output: ioslides_presentation
subtitle: Github, Rstudio, Experimental Design, Statistical Power
---
## Lets Begin

Science is built with facts as a house is with stones – but a collection of facts is no more a science than a heap of stones is a house.

					– Jules Henri Poincare

## What is R

R is a scripting language so we can just run this and it doesn't need to get compiled into any other form.  It is arguably the primary language used by statistics, biostatistics, and statistical genetics.  Alternatives to this can be SPSS, SAS, Python, and Julia.  You can use whatever you want but all code I will be presenting will be in R  

## R Studio

R Studio is a development enviornment for coding in R, I highly recommend it.  

![R studio](Rstudio_ScreenGrab.png)

## Github

GitHub is a Web-based Git repository hosting service. It offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. Unlike Git, which is strictly a command-line tool, GitHub provides a Web-based graphical interface and desktop as well as mobile integration. It also provides access control and several collaboration features such as bug tracking, feature requests, task management, and wikis for every project.

## Github

We will use github for pulling down data and sharing.  Please make an account at github.com

## Sites to get help

Don’t go at this stuff alone!
I am always available for help
However, one of the best places to go is Q and A sites.  

## Biostars

Biostars is a stackexchange style site for questions and answers to get help on anything from bioinformatics to statistical genetics.  Questions are answered quickly.  

![biostars](biostars.png)

##Central Dogma of Statistics

![Central Dogma](centralDogmaOfStats.jpg)

## Axioms of probability


Let A be an event defined over probability space S.  Then $P(A) \geq 0$

$P(S) = 1$

Let A and B be any two mutually exclusive events defined over S.  Then:

$$
P(A \cup B) = P(A) + P(B)
$$

## Quick overview of probability and statistics

$P(A^c) = 1 - P(A)$

By second Axiom, $P(S) = 1 = P(A \cup A^c)$

Since $A$ and $A^c$ are mutually exclusive then we can use third axiom.  

$$
1 = P(A \cup A^c) = P(A) + P(A^c) \\
1 = P(A) + P(A^c) \\
1 - P(A) = P(A^c)
$$

## Conditional Probability

Conditional Probability is the idea that we want to ask about one probability's event given that another event has occured.  

Let $A$ and $B$ be any two events defined on $S$ such that $P(B) > 0$.  The conditional probability of $A$, assuming that $B$ has occured is written $P(A|B)$.  It is given by:

$$
P(A|B) = \frac{P(A\cap B)}{P(B)}
$$

## Random Variable 

a random variable is a variable whose value is subject to variations due to chance.  A random variable can take on a set of possible different values.  The set of values this can be is usually referred to as the sample space.  

## Expected Value

The expected value of a random variable is intuitively the long-run average value of repetitions of the experiment it represents.  

The law of large numbers guarantees that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions goes to infinity.

## Expected Value

$$
E(X) = \sum_{k}k\frac{1}{n} = \frac{1}{n}\sum_{all \, k}k
$$

Example: Dice roll

$$
\begin{align}
E(X) & = \frac{1}{6}\sum_{i=1}^{6}i = \frac{1}{6}(1+2+3+4+5+6) \\
& = \frac{1}{6}(21) = \frac{21}{6} = 3.5
\end{align}
$$

## Expected Value

This can also be written as:
$$
E(X) = \mu = \mu_X = \sum_{all \, k}k\cdot p_X(k)
$$

Notice this is only for the discrete case.  

## Continuous expected value

If Y is a continuous random variable with probability density function (pdf) $f_Y(y)$ then the expected value is:

$$
E(X) = \mu = \mu_X = \int_{-\infty}^{\infty}y\cdot f_Y(y)dy
$$

Assuming both the sum and the integral converge absolutely.  This creates some issues that I think can be resolved but for our purposes we want them to converge (You can do some crazy stuff with dirac delta functions and other distributions that get around classic functions).  To converge absolutely, we want:

$$
\sum_{all \, k}k\cdot p_X(k) < \infty  \int_{-\infty}^{\infty}y\cdot f_Y(y)dy < \infty
$$

## Variance

The variance of a random variable is the expected value of its squared deviations from $\mu$.  If X is discrete with pdf $p_X(k)$

$$
Var(X) = \sigma^2 = E\left[(X - \mu)^2\right] = \sum_{all \, k}(k-\mu)^2\cdot p_X(k)
$$
If Y is continuous with pdf $f_Y(y)$
$$
Var(Y) = \sigma^2 = E\left[(Y - \mu)^2\right] = \int_{-\infty}^{\infty}(y-\mu)^2\cdot f_Y(y)dy
$$

## Standard Deviation

## Z transformation

While we won't be doing the proof here, we will be using what is called the Z transformation throughout this class.  For a continuous random variable $Y$, mean $\mu$ and variance $\sigma^2$, then we can let:
$$
  \frac{Y-\mu}{\sigma} = Z
$$

What value does this have?  What we really want is to show that this is also a standard normal random variable.  

## Statistical Moments

The $n^{th}$ moment of a real valued continuous function $f(x)$ of a real variable about a value $c$ is 

$$
\mu_n =  \int_{-\infty}^{\infty}(x-c)^nf(x)dx
$$

For nth moments about zero of a probability density function $f(x)$ is the expected value of $X^n$.  Moments about the mean $\mu$ are called central moments.  

For $n = 1$, the raw moment is the mean.

For $n = 2$, the central moment is the variance.  

So as we can see, the moments are very powerful in gaining incite about what we are trying to do. 

## Moment Generating Functions

Finding moments directly of random variables, especially for higher moments can be very difficult.  Thus, we will use moment generating functions, which have the very useful property that the $rth$ derivative of $M_W(t) evaluated at zero is equal to $E(W^r)$

Let W be a random variable.  The moment generating function (mgf) for $W$ is denoted $M_W(t)$ and given by:

$$
M_W(t) = E(e^{tW}) = 
\begin{cases}
\sum_{all \, k}e^{tk}p_W(k),& \text{if W is discrete} \\
\int_{-\infty}^{\infty}e^{tW}f_W(w)dw, & \text{if W is continuous}
\end{cases}
$$

At all values of $t$ for which the expected value exists


## Cumulative Distribution Function

In working with random variables, we frequently need to calculate the probability that the value of a random variable is somewhere between two numbers.  

For example, suppose we want to evaluate the expression $P(s\leq X \leq t)$.  If the pdf for this is $p_X(k)$ for $X$ then:  

$$
P(s\leq X \leq t) = \sum_{k=s}^tp_X(k)
$$

But this can be difficult depending on how many $k$'s there are and what our pdf is.  We can also write this:

$$
P(s\leq X \leq t) = P(X\leq t) - P(X \leq s - 1)
$$

## Proof of Z transformation is standard normal.  

We need to show that the random variable $Z$ follows a $N(0,1)$ distribution. So, let's find the cumulative distribution function $F(z)$, which is also incidentally referred to as $\Phi(z)$
$$
Z = \frac{X-\mu}{\sigma}
$$
So
$$
F_Z(z) = \Phi(z) = P(Z \leq z) = P\left(\frac{X-\mu}{\sigma} \leq z\right)
$$

## Remember defintion of Standard Normal

$$
P(X \leq x) = \int_{-\infty}^{x}\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-x^2}{2}}dx
$$

## Proof of Z transformation is standard normal

So lets apply our definition of standard normal to the Z transformation

$$
F_X(z) = P(X\leq\mu+z\sigma) = \int_{-\infty}^{\mu +z\sigma}\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-(\frac{x-\mu}{\sigma})^2}{2}}dx
$$

Lets do another substitution.  Let $w=\frac{x-\mu}{\sigma}$.  From this we will get:

$$
x = w\sigma \\
dx = \sigma dw
$$

## Proof of Z transformation is standard normal

Now lets apply our w substitution.  

For the endpoints, if $x = −\infty$, then $w$ also equals $−\infty$; and if $x = \mu + z\sigma$, then $w = \frac{\mu + z\sigma − \mu}{\sigma} = z$. Therefore, upon making all of the substitutions for $x$, $w$, and $dx$, our integration looks like this:

$$
\begin{align}
F_X(z) & = \int_{-\infty}^{z}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{w^2}{2}}\sigma dw \\
& = \int_{-\infty}^{z}\frac{1}{\sqrt{2\pi}}e^{-\frac{w^2}{2}} dw
\end{align}
$$

## Proof of Z transformation is standard normal

We should now recognize that as the cumulative distribution of a normal random variable with mean $\mu = 0$ and standard deviation $\sigma = 1$. Our proof is complete.

## Similiar proof for Z transformation

A similar proof can be found here:
http://aubuchon.jsc.vsc.edu/probTheoryDocumentTheoremNormTrans.htm


##Variability of biological experiment

V(Genomic Measurement) = 
Phenotypic variability
+ Measurement Error + Natural biological variation

What does this mean?

## Experimental Design Issues

In biology, traditional approaches to inquiry involved hypothesis testing.
We identify a problem and postulate a mechanism
we design an experiment in which we perturb the system and then look for changes.  The response of the system either validates or invalidates our hypothesis

In these types of experiments, we attempt to tightly control the variables so as to carefully measure the influence of these, perturbing a single parameter at a time

Good experimental design requires sufficient replication to estimate the effects we wish to measure

## Basis of Experimental Design

Functional genomics technologies have dramatically changed the way in which we approach biological questions.

We can now survey the responses of thousands of genes, proteins, or metabolites in a particular system and look for patterns of expression
These “hypothesis generating” experiments do not (necessarily) require a mechanistic hypothesis ahead of time. However, this does not mean we do not have to carefully design our experiment and analyze the data.

Here, we attempt to control the variables so as to carefully measure the influence of these, perturbing a single parameter at a time

Good experimental design requires sufficient replication to estimate the effects we wish to measure

## Types of Experiments

**Class Comparison**  
Can I find genes that distinguish between two classes, such as tumor and normal?

**Class Discovery**  
Given what I think is a uniform group of samples, can I find subsets that are biologically meaningful?

**Classification**  
Given a set of samples in different classes, can I assign a new, unknown sample to one of the classes?

**Mechanistic Studies**  
Can I discover a causative mechanism associated with the distinction between classes?

These are often not completely distinct and a single dataset can often be used for multiple purposes

## Class Discovery

The Question: Given data on a collection of samples, are there biologically interesting patterns that exist?

Unsupervised methods do not use the sample classification as input – they do not take into account, for example, whether the samples come from ALL(Acute lymphoblastic leukemia) or AML (acute myeloid leukemia) patients. 

They simply group samples together based on some measure of similarity between them. 

Useful approaches to data analysis are unsupervised methods such as hierarchical clustering or k-means clustering.

## Class Comparison

The Question: Given a collection of samples that can be grouped into two or more classes, can we find patterns of gene expression that distinguish them?

Analysis methods typically rely on statistical comparisons, such as t-test or ANOVA to find genes whose patterns are significantly correlated with the various classes.

These supervised methods use the sample classification as input – we might compare, for example, gene expression differences between ALL and AML patients. 

The effectiveness of these analyses increase as we increase our sample size.

## Class Prediction

The Question: Given samples that I can know belong to various groups, can I find a rule that will allow me to assign new samples to one of the groups?

There are a wide range of algorithms that can be used for class prediction, including weighted voting, k Nearest Neighbors (kNN) and Artificial Neural Networks.

One must have sufficient sample numbers to allow an algorithm to be trained and then to have it tested on an independent set of samples.

Often sample sources are limited and one uses cross validation.

## Mechanistic Analysis

The Question: Given a collection of samples representing various phenotypes, can I find a biological mechanism that provides a mechanistic basis for the observed differences?

This often involves a meta-analysis of the data, bringing in other information such as gene functional assignments to gain insight.

There are a wide range of software tools that have been developed such as EASE, GO Miner, and GenMAPP that  can help put the results into context.

These experiments are often hypothesis testing experiments that use microarrays as the readout.

## The relationship between types

All of these various experimental types are related and often are used together.

Class discovery methods are often used as a first analysis of any experiment as a means of quality control.

Nearly every experiment involves a comparison between classes representing different phenotypes or treatments.

The start of class prediction is class comparison to find genes that distinguish the various classes.

Mechanistic analysis begins with gene selection and the best classification genes are those that explain the mechanism underlying the classes.

## The Experimental Design

The Experimental Design dictates a good deal of what you can do with the data.

Good normalization and processing reflects the experimental design.

The design also facilitates certain comparisons between samples and provides the statistical power you need for assigning confidence limits to individual measurements.

The design must reflect experimental reality.

The most straight-forward designs compare expression in two classes of samples to look for patterns that distinguish them.

## Normal Distribution
```{r}
x <- seq(-4, 4, length=100)
hx <- dnorm(x)
degf <- c(1, 3, 8, 30)
colors <- c("red", "blue", "darkgreen", "gold", "black")
labels <- c("df=1", "df=3", "df=8", "df=30", "normal")
```

## Normal Distribution

What is special about this distribution?
```{r}
plot(x, hx, type="l", lty=2, xlab="x value",ylab="Density", 
     main="Comparison of t Distributions")
for (i in 1:4){lines(x, dt(x,degf[i]), lwd=2, col=colors[i])}
legend("topright", inset=.05, title="Distributions",labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)
```


## T test

In this we are going to be looking to use the T test to work with.  The T test is a statistical tool that is used to see if two data sets are statistically different from one another and is used when the test statistic follows roughly a normal distribution.  Another way of saying this is to see if two means are reliably different from each other.  

## T test
For example:
```{r}
cars <- c(52,49)
barplot(cars, main="coin flips for heads",names=c("me","you"),xlab="flips by person",ylab="number of heads")
```

## T test
We can see here that I get more heads than you, but do I reliably get more heads than you?  That is what the t test hopes to do.  This is an inferencial statistic.  The T test is going to measure the variance between the groups divided by the variance within these groups.  Lets assume there is a difference of 6 within these groups, thus giving a t value of 1/3.  This isn't big enough to give us 

## T test
We can see here that I get more heads than you, but do I reliably get more heads than you?  That is what the t test hopes to do.  This is an inferencial statistic.  The T test is going to measure the variance between the groups divided by the variance within these groups.  Lets assume there is a difference of 6 within these groups, thus giving a t value of 1/3.  This isn't big enough to give us 

Each T test has a corresponding P value.  This P value is the probability that the pattern of data in the sample could be produced by random data.  If the p value is 0.5, then that means there is a 5% chance there is no real difference.  In general, we only accept p = 0.5 as being reliable.

In general we want a p value for our t test of 0.05 or below.  

## T test
Exact p value depends on the sample size.  The p-value for each t-value also depends on the sample size. Bigger samples make it easier to detect differences.  

Example: A company that manufactures light bulbs claims that a particular type of light bulb will last 850 hours on average with standard deviation of 50. A consumer protection group thinks that the manufacturer has overestimated the lifespan of their light bulbs by about 40 hours. How many light bulbs does the consumer protection group have to test in order to prove their point with reasonable confidence?

## T Test Example
Our first goal is to figure out the number of light bulbs that need to be tested.  That is, we will determine the sample size for a given a significance level and power. Next, we will reverse the process and determine the power, given the sample size and the significance level.

## T Test Example

We know so far that the manufacturer claims that the average lifespan of the light bulb is 850 with the standard deviation of 50, and the consumer protection group believes that the manufactory has overestimated by about 40 hours. So in terms of hypotheses, our null hypothesis is $H_0 = 850$ and our alternative hypothesis is $H_a = 810$.

## T Test Example

The significance level is the probability of a Type I error (i.e. $\alpha$), that is the probability of rejecting $H_0$ when it is actually true. We will set it at the .05 level. The power of the test against Ha is the probability of that the test rejects $H_0$. We will set it at .90 level.

## T Test Example

We are almost ready for our power analysis. But let's talk about the standard deviation a little bit.

Intuitively, the number of light bulbs we need to test depends on the variability of the lifespan of these light bulbs. Take an extreme case where all the light bulbs have exactly the same lifespan. Then we just need to check a single light bulb to prove our point. Of course, this will never happen. 

On the other hand, suppose that some light bulbs last for 1000 hours and some only last 500 hours. We will have to select quite a few of light bulbs to cover all the ground. Therefore, the standard deviation for the distribution of the lifespan of the light bulbs will play an important role in determining the sample size.

Power Analysis

In R, it is fairly straightforward to perform a power analysis for comparing means. For example, we can use R's pwr.t.test function for our calculation as shown below. First, we specify the two means, the mean for the null hypothesis and the mean for the alternative hypothesis divided by the standard deviation for the population. We also need to set the alpha level (.05 for our example), type equal to one.sample and alternative equal to two.sided (two-tail).

## T Test Example

Power Analysis

In R, it is fairly straightforward to perform a power analysis for comparing means. For example, we can use R's pwr.t.test function for our calculation as shown below. First, we specify the two means, the mean for the null hypothesis and the mean for the alternative hypothesis divided by the standard deviation for the population. We also need to set the alpha level (.05 for our example), type equal to one.sample and alternative equal to two.sided (two-tail).

## T Test Example

```{r}
library(pwr)
pwr.t.test(d=(850-810)/50,power=0.9,sig.level=0.05,type="one.sample"
           ,alternative="two.sided")
```

## T Test Example

The result tells us that we need a sample size at least 19 light bulbs to reject $H_0$ under the alternative hypothesis $H_a$ to have a power of 0.9.

Next, suppose we have a sample of size 10, how much power do we have keeping all of the other numbers the same? 
```{r}
pwr.t.test(d=(850-810)/50,n=10,sig.level=0.05,type="one.sample"
           ,alternative="two.sided")
```

## T Test Example

You can see that the power is about .616 for a sample size of 10. What then is the power for sample size of 15?

```{r}
pwr.t.test(d=(850-810)/50,n=15,sig.level=0.05,type="one.sample"
           ,alternative="two.sided")
```
So now the power is about .82. You could also do it again to find out the power for a sample size of 20. You'll probably expect that the power will be greater.

## T Test Example

So if we want 80% power if looks like 15 light bulbs will do the trick.  

There is another technical assumption, the normality assumption. If the variable is not normally distributed, a small sample size usually will not have the power indicated in the results, because those results are calculated using the common method based on the normality assumption. It might not even be a good idea to do a t-test on such a small sample to begin with if the normality assumption is in question.

## Sample Size and Power Calculations

What is Statistical Power?  

Statistical Power refers to the is the likelihood that a study will detect an effect when there is an effect there to be detected. If statistical power is high, the probability of concluding there is no effect when, in fact, there is one, goes down.

* Power = Probability of discovering a real signal if it is there  
* Power is typically set to 80%  
* Calculations are based on made up assumptions (since you need to calculate this often before experiment is run)  
* Higher power is better  
* Lower power studies don’t replicate




## Picturing statistical power

study 1: 263 cases, 1241 controls
![Case 1](case1_part1.jpg)

## Picturing statistical power

study 1: 263 cases, 1241 controls
![Case 1](case1_part2.jpg)

## Picturing statistical power

study 1: 263 cases, 1241 controls
![Case 1](case1_part3.jpg)

## More on Power

Study 2: 18 treated, 72 controls, STD DEV = 2
![Case 2](case2_part1.jpg)

## More on Power

Study 2: 18 treated, 72 controls, STD DEV=10
![Case 2](case3_part1.jpg)

## More on Power

Study 2: 18 treated, 72 controls, effect size=1.0
![Case 3](case4_part1.jpg)

## More on Power
Bigger difference from the null mean.
![Bigger Difference](biggerDifference.jpg)

## More on Power
Bigger standard deviation
![Bigger Standard Deviation](biggerStandardDeviation.jpg)

## More on Power
Higher Significance Level
![Higher Significance Level](HigherSignificance.jpg)

## Sample Size Calculations

Based on these elements, you can write a formal mathematical equation that relates power, sample size, effect size, standard deviation, and significance level.  

We will derive this.  

## Deriving Z-Test Formulas: 1-Sample, 1-Sided

Suppose we have random variables

$$Y_1,Y_2,\ldots,Y_n$$

And we want to compare the hypothesis:

$$H_0:\mu=\mu_0$$
$$H_1:\mu>\mu_0$$

Where $\mu$ is the mean of the distribution and $\mu_0$ is a specified comparison value.  

## Deriving Z-Test Formulas: 1-Sample, 1-Sided

The test statistic will be the sample average.  

$$X = \bar{Y} = \frac{1}{n}\sum_{i=i}^{n}Y_i$$

## Deriving Z-Test Formulas: 1-Sample, 1-Sided

Since we're testing whether the mean is larger than $\mu_0$, we'll reject the null hypothesis if $X$ turns out to be "too big". 

We'll need a critical value, $cv$ that serves as the cut-off point; that is, if our observed value of $X$ turns out to be larger than cvcv, then we'll reject the null hypothesis. If cvcv is very large, then it'll be very unlikely to observe a value of $X$ larger than $cv$. On the other hand, if $cv$ is too small, i.e. too close to $\mu_0$, then it'll be much more likely to observe a value of $X$ larger than $cv$.

## Deriving Z-Test Formulas: 1-Sample, 1-Sided

So, would we rather have a large or small critical value, and how do we decide? This is based on our desired Type I error rate, $\alpha$, which is often set at the familiar value of 0.05, or 5%. Specifically, if the null hypothesis is true, we want the probability of rejecting the null hypothesis to be αα. We can write this in symbols as:

$$\alpha = P(X \geq cv \,|\, H_0)$$

## Deriving Z-Test Formulas: 1-Sample, 1-Sided

Using some algebra:

$$
\begin{aligned}
\alpha & = P(X \geq cv \,|\, H_0) \\
& = 1 - P(X \leq cv \,|\, H_0)
\end{aligned}
$$
Since probabilities sum to one.
$$
= 1 - P\left(\frac{X-\mu_0}{\sigma_n} \leq \frac{cv-\mu_0}{\sigma_n}\,|\, H_0\right)
$$

## Remember

If a random variable is normally distributed with mean $\mu$ and variance $\sigma^2$, then subtracting $a$ makes the mean $\mu-a$, and dividing by $b$ makes the variance $\sigma^2/b^2$.  We can use this to our advantage.  

$$X \sim N(\mu,\sigma^2) \implies \frac{X-\mu}{\sigma} \sim N(0,1) $$

Using this, we can then get the following:
$$
\begin{align}
&\alpha = 1 - P\left(\frac{X-\mu_0}{\sigma_n} \leq \frac{cv-\mu_0}{\sigma_n}\,|\, H_0\right) \\
& = 1 - \Phi\left(\frac{cv-\mu_0}{\sigma_n}\right)
\end{align}
$$

## Deriving Z-Test Formulas: 1-Sample, 1-Sided

So, 

$$
\alpha  = 1 - \Phi\left(\frac{cv-\mu_0}{\sigma_n}\right)  \\
1 - \alpha = \Phi\left(\frac{cv-\mu_0}{\sigma_n}\right)
$$

Using definition of standard normal quantile:

$$
z_{1-\alpha} = \frac{cv-\mu_0}{\sigma_n} \\
cv = \mu_0 + z_{1-\alpha}\sigma_n
$$

## Critical Value Result

Acceptance Region: Accept $H_0$ if $X<\mu_0+z_{1−\alpha}\sigma_n$   
Rejection Region: Reject $H_0$ if $X\geq\mu_0+z_{1−\alpha}\sigma_n$

Note also that:

Let $z_a$ be the $a^{th}$ quantile of the standard normal distribution. That is, the area under the standard normal curve to the left of $z_a$ is $a$.  Then

$$
z_a = -z_{1-a}
$$

## Power Derivation

$$ 
\begin{align}
Power & = P(X\geq\mu_0 + z_{1-\alpha}\sigma_n \,|\, H_1) \\
& = 1 - P(X<\mu_0 + z_{1-\alpha}\sigma_n \,|\, H_1) \\
& = 1 - P\left(\frac{X-\mu}{\sigma_n}<\frac{\mu_0 + z_{1-\alpha}\sigma_n -\mu}{\sigma_n} \,|\, H_1\right) \\
& = 1- \Phi\left(\frac{\mu_0-\mu}{\sigma_n} + z_{1-\alpha}\right) \\
\end{align}
$$

And using the symmetry here of $1-\Phi(x) = \Phi(-x)$ or $z_a = -z(1-a)$

$$
Power  =  \Phi\left(\frac{\mu_0-\mu}{\sigma_n} - z_{1-\alpha}\right)
$$

## Type I and Type II Error

In statistical hypothesis testing, a type I error is the incorrect rejection of a true null hypothesis (a "false positive"), while a type II error is the failure to reject a false null hypothesis (a "false negative"). More simply stated, a type I error is detecting an effect that is not present, while a type II error is failing to detect an effect that is present.

## Sample Size

A formula for sample size can be obtained by algebraically solving for nn in the above power formula. Here, we'll solve for $\sigma_n$, and below we'll fill in the specific form of σnσn for the cases when the data are normal and binomial.  First, note that the typical notation is Power = $1-\beta$, where $\beta$ is the Type II error rate. First lets take the power formula from before:

$$
1-\beta = \Phi\left(\frac{\mu-\mu_0}{\sigma_n}-z_{1-\alpha}\right) \\
$$
Then apply our definition of standard normal quantiles
$$
z_{1-\beta} = \frac{\mu-\mu_0}{\sigma_n}-z_{1-\alpha}
$$

## Sample Size

Then some algebra:

$$
\frac{1}{\sigma_n} = \frac{z_{1-\beta} + z_{1-\alpha}}{\mu-\mu_0}
$$

## Normal Data

Now suppose the data $Y_1,Y_2,...,Y_n$ are independently and identically distributed (iid) to $N(\mu,\sigma^2)$.  Let $X=\bar{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i=E(Y)$.  

Then we know the expected value is similiarly distributed.  (We will not prove this).  So $\bar{Y} \sim N(\mu,\sigma^2/n)$

Thus we replace $\sigma_n$ with $\sigma/\sqrt{n}$ and use our standard normal identity and power equation:

$$
Power = \Phi\left(\frac{\mu - \mu_0}{\sigma/\sqrt{n}}-z_{1-\alpha}\right) \\
n = \left(\sigma\frac{z_{1-\beta}+z_{1-\alpha}}{\mu-\mu_0}\right)^2
$$

## Binomial Data

Binomial data is a series of binary outcomes (coin flips).  

Suppose the data $Y_1,Y_2,...,Y_n$ represent $n$ independent binary outcomes, each with success probability $p$.  Then the test statistic is the sample proportion, $X = \hat{p}=\frac{1}{n}\sum_{i=1}^{n}Y_i$.  For large n, we get the approximate distribution (you guessed it)

$$
\hat{p} \sim N\left(p,\frac{p(1-p)}{n}\right)  
$$

So we take our previous sample size and power calculations and replace $\sigma_n$ with $\sqrt{p(1-p)/n}$

## Binomial Data

We are left with

$$
Power = \Phi\left(\frac{p-p_0}{\sqrt{p(1-p)/n}}-z_{1-\alpha}\right)
$$

and

$$
n=p(1-p)\left(\frac{z_{1-\beta}+z_{1-\alpha}}{p-p_0}\right)^2
$$

## Quantile

Please note $z_a$ is the quantile of the standard normal distribution such that the area under the curve to the left of $z_a$ is $a$. These quantiles are easily obtained from a standard normal distribution table or programatically. For example, if $\alpha=0.05$, then we have the familiar values of $z_{1−\alpha}=1.64$ and $z_{1−\alpha/2}=1.96$.

## Example Sample Size Calculation

How many people would you need to sample in each group to achieve power of 80%?  Assume our mean is 120 and we are comparing to 115 and standard deviation is 24.  

## Example 1 Solution

So remember, $\beta$ is our type 2 error.  This means $1-\beta$ is our power.  Since our power is 80%, that means $\beta$ is 20%.  Thus we have:
$$
\begin{align}
n & = \left(\sigma\frac{z_{1-\beta}+z_{1-\alpha}}{\mu-\mu_0}\right)^2 \\
& = \left(24\frac{0.84+1.64}{120-115}\right)^2 \\
& = \left(\frac{59.52}{5}\right)^2 = 141.7052 
\end{align}
$$



## Example 2 Sample Size

What power can I achieve if my our mean is 100 and we are comparing to 90 and standard deviation is 10 with 12 samples?

## Example 2 Sample Size Solution

$$
\begin{align}
Power & = \Phi\left(\frac{\mu - \mu_0}{\sigma/\sqrt{n}}-z_{1-\alpha}\right) \\
& = \Phi\left(\frac{100 - 90}{10/\sqrt{12}}-1.64\right) \\
& = \Phi(\frac{10\sqrt(12)}{10} - 1.64) = \Phi(1.82) = 0.9656
\end{align}
$$

## Loading data

Then we are going to load data in.  A lot of data is loaded from file formats

```{r}
mydata = read.table("dummyTable.txt")
mydata
```


## Loading data

We can also check the class for an object, in this case it will be a data frame

```{r}
class(mydata)
```

As we can see, our table has headers:
```{r}
mydata <- read.table("dummyTable.txt",header = TRUE)
mydata
```


## Manipulating Data

Notice now all of our data has nice headers.  And we can index off of them. This will give us a data frame. 

```{r}
mydata["val1"]
```

This will give us an array.  

```{r}
mydata$val1
```

## Operations

We can do basic vector math

```{r}
a <- c(1,2,3)
b <- c(10,11,12,13)
a+b
```

Other vector operations

```{r}
min(a,b)
max(a,b)
```

## R example of sample size calculation

In our previous sample size calculation,we saw we got 142 as the sample size.  Lets see if we we can do this now in r.  

```{r}
mu=115
mu0=120
sd=24
alpha=0.05
beta=0.20
(n=(sd*(qnorm(1-alpha)+qnorm(1-beta))/(mu-mu0))^2)
```
So,  because of our errors i rounding, we are actually a full sample off.  

## R example of sample size calculation

And as we expect

```{r}
z=(mu-mu0)/sd*sqrt(n)
(Power=pnorm(abs(z)-qnorm(1-alpha)))
```

## R example of power calculation

Remember the question is: What power can I achieve if my our mean is 100 and we are comparing to 90 and standard deviation is 10 with 12 samples?

Lets set up our variables

```{r}
alpha=0.05
mu = 100
mu0 = 90
sd = 10
n = 12
Power = pnorm((100-90)/10*sqrt(12)-qnorm(1-alpha))
Power
```


## What did we just do here?

So we just used the pnorm and qnorm functions.  What do these do?

We can run the question mark command to get the R documentation on a command

```{r}
?pnorm
```

As we see when we go to it, we see pnorm returns the cdf value and qnorm returns the inverse cumulative density function (quantiles).  

Good read up on this is at: http://www.cyclismo.org/tutorial/R/probability.html


## How Many Replicates?

From Simon et al., Genetic Epidemiology 23: 21-36, 2002)

$$
n = \frac{[7.84\sigma^2(z_{a/2} + z_{b})^2]}{\delta^2}
$$

Where $z_{a/2}$ and $z_b$ are normal percentile values at significance level a and false negative rate $\beta$; parameter $\delta$ represents the minimum detectable log2 ratio; and $\sigma$ represents the SD of log ratio values.

For $\alpha = 0.001$ and $\beta = 0.05$, then $z_{a/2} = -3.29$ and $z_b = -1.65$.
Assume $\delta = 1.0$ (2-fold change) and $\sigma = 0.25$,
Therefore $n = 12$ samples (6 query and 6 control).  This is clearly not sufficient.


## Power Calculations

* There are many papers now describing microarray power calculations.

* Most require knowledge of many parameters that simply are not known and hard to estimate given our lack of knowledge of the existing data

* Most suggest 4-6, 20-30, >100 samples per group for most two-class experiments but these are often insufficient, particularly for applications such as classification

* The real calculation people do is:	
$$
\frac{money\,available}{money\,per\,array}
$$

This is an area where significant work remains to be done.  Answers depends on the goal.

## Replication

* People distinguish “Biological Replicates” and “Technical Replicates”

* “Technical Replicates” hybridize the same samples to the array multiple times – captures variability in the assay

* “Biological Replicates” use independent biological samples -  captures variability in biological system and in the assay

